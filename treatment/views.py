

import logging
import google.generativeai as genai
from django.conf import settings
import json
from rest_framework.views import APIView
from rest_framework.parsers import MultiPartParser, FormParser
from rest_framework import status
from rest_framework.response import Response
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from datetime import datetime
from rest_framework.permissions import IsAuthenticated
from .models import CorrectionHistory  , ChatMessage  , ChatSession , ImageCorrection# Import du mod√®le
from .serializers import CorrectionHistorySerializer , ChatMessageSerializer , ChatSessionDetailSerializer , ImageCorrectionSerializer
from subscriptions.models import   UsageLog , Subscription   # Import du mod√®le

import google.generativeai as genai
from django.conf import settings
from django.shortcuts import get_object_or_404
from rest_framework import generics, status
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated


from rest_framework.decorators import api_view, schema




# backend/views.py
import google.generativeai as genai
from django.conf import settings
from rest_framework import generics, status
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated




# backend/views.py
# backend/views.py
from rest_framework import generics, status
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated
from django.shortcuts import get_object_or_404
from django.utils import timezone
from .models import ChatSession, ChatMessage
from .serializers import ChatSessionDetailSerializer , ChatMessageSerializer
from django.conf import settings



logger = logging.getLogger(__name__)
genai.configure(api_key=settings.GEMINI_API_KEY)

class ProcessImageView(APIView):
    parser_classes = [MultiPartParser, FormParser]
    permission_classes = [IsAuthenticated]

    LITERARY_DOMAINS = [
        'Fran√ßais', 'Histoire-G√©ographie', 'Philosophie', 
        'Langues √©trang√®res', 'Autre'
    ]
    
    SENSITIVE_TYPES = [
        'Exercice de r√©daction', 'Analyse de texte'
    ]
    
    SCIENTIFIC_DOMAINS = [
        'Math√©matiques', 'Physique-Chimie', 'SVT', 
        'Informatique', '√âconomie / SES'
    ]
    EDUCATIONAL_LEVELS = [
        'Lyc√©e ‚Äì Terminale', 'Lyc√©e ‚Äì Premi√®re', 'Lyc√©e ‚Äì Seconde',
        'Coll√®ge (6·µâ ‚Äì 3·µâ)', 'Sup√©rieur ‚Äì BTS / DUT', 'Sup√©rieur ‚Äì Licence'
    ]

    def post(self, request):
        TEST_MODE = False
        if TEST_MODE:
            image_path = r'C:\git_project\CORRECTION APP BACKEND\treatment\images\testsvt.jpg'
            context_str = json.dumps({
                'domaine': 'G√©n√©ral',
                'niveau': 'Lyc√©e ‚Äì Terminale',
                'type_exercice': 'Probl√®me √† r√©soudre',
                'attente': 'Solution √©tape par √©tape',
                'infos': ''
            })
            try:
                with open(image_path, 'rb') as f:
                    image_bytes = f.read()
            except FileNotFoundError:
                logger.error(f"Image locale non trouv√©e: {image_path}")
                return Response(
                    {'success': False, 'message': f'Image locale non trouv√©e: {image_path}'},
                    status=status.HTTP_400_BAD_REQUEST
                )
        else:
            image = request.FILES.get('image')
            context_str = request.POST.get('context')

            if not image:
                return Response(
                    {'success': False, 'message': 'Aucune image fournie.'},
                    status=status.HTTP_400_BAD_REQUEST
                )
            image_bytes = image.read()

        try:
            context = json.loads(context_str) if context_str else {}
            domaine = context.get('domaine', 'Math√©matiques')
            niveau = context.get('niveau', 'Coll√®ge (6·µâ ‚Äì 3·µâ)')
            type_exercice = context.get('type_exercice', 'Probl√®me √† r√©soudre')
            attente = context.get('attente', 'Solution √©tape par √©tape')
            infos = context.get('infos', '')

            logger.info(f"Contexte: Domaine={domaine}, Type={type_exercice}, Niveau={niveau}")

            # EXTRACTION INITIALE DU TEXTE POUR FALLBACK
            extracted_text = self._extract_text(image_bytes)

            # ANALYSE INITIALE DE LA BRANCHE PAR L'IA
            branch_analysis = self._detect_branch(image_bytes, extracted_text)

            # V√âRIFICATION DE L'INCOH√âRENCE DOMAINE/BRANCHE
            detected_branch = branch_analysis['branch']
            detected_domain = branch_analysis['detected_domain']
            domain_mismatch = None
            if domaine != detected_domain:
                domain_mismatch = (
                    f"L'utilisateur a indiqu√© '{domaine}', mais l'exercice est d√©tect√© comme "
                    f"'{detected_domain}' ({detected_branch})."
                )

            # ANALYSE DU TYPE DE CONTENU
            content_analysis = self._analyze_content_type_with_education(
                detected_domain, type_exercice, attente, niveau
            )

            ia_response = self.call_gemini_api(
                image_bytes, detected_domain, niveau, type_exercice,
                attente, infos, content_analysis
            )

            # ‚Üê CORRECTION : extraire le texte AVANT de construire response_data
            extracted_text_final = ia_response.get('extracted_text', extracted_text)

            # DATE/HEURE BACKEND
            current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # CONSTRUCTION DE LA R√âPONSE
            response_data = {
                'success': True,
                'data': {
                    'user_domain': domaine,
                    'user_level': niveau,
                    'user_exercise_type': type_exercice,
                    'user_expectation': attente,
                    'user_info': infos,
                    'detected_branch': detected_branch,
                    'detected_branch_explanation': branch_analysis['explanation'],
                    'domain_mismatch': domain_mismatch,
                    'response_datetime': current_datetime,
                    'extracted_text': extracted_text_final,          # ‚Üê S√õR
                    'solution': {
                        'result': ia_response.get('result', ''),
                        'steps': ia_response.get('steps', [])
                    }
                },
                'content_type': content_analysis['type'],
                'educational_mode': content_analysis.get('educational_mode', False)
            }

            # GESTION DES ERREURS DE GEMINI (ex. COPYRIGHT_BLOCK)
            if not ia_response.get('success', True):
                if ia_response.get('error_type') == 'COPYRIGHT_BLOCK':
                    ia_response = self._educational_fallback(
                        detected_domain, type_exercice, niveau, content_analysis
                    )
                    # Recalcul du texte en mode fallback
                    extracted_text_final = ia_response.get(
                        'extracted_text',
                        f"Mode p√©dagogique - {domaine}"
                    )

                if not ia_response.get('success', True):
                    response_data = {
                        'success': False,
                        'message': ia_response['message'],
                        'data': {
                            'user_domain': domaine,
                            'user_level': niveau,
                            'user_exercise_type': type_exercice,
                            'user_expectation': attente,
                            'user_info': infos,
                            'solution': None,
                            'detected_branch': detected_branch,
                            'detected_branch_explanation': branch_analysis['explanation'],
                            'domain_mismatch': domain_mismatch,
                            'extracted_text': extracted_text_final  # ‚Üê Toujours pr√©sent
                        },
                        'content_type': content_analysis['type'],
                    }

            # SAUVEGARDE DANS L'HISTORIQUE (utilise extracted_text_final)
            CorrectionHistory.objects.create(
                user=request.user,
                user_domain=domaine,
                user_level=niveau,
                user_exercise_type=type_exercice,
                user_expectation=attente,
                user_info=infos,
                detected_branch=detected_branch,
                detected_branch_explanation=branch_analysis['explanation'],
                domain_mismatch=domain_mismatch,
                response_datetime=current_datetime,
                extracted_text=extracted_text_final,               # ‚Üê CORRECTION
                solution=response_data['data'].get('solution'),   # .get() pour √©viter KeyError
                content_type=content_analysis['type'],
                educational_mode=content_analysis.get('educational_mode', False),
                success=response_data['success'],
                error_message=response_data.get('message')
            )

            return Response(response_data)

        except json.JSONDecodeError:
            # ‚Üê CORRECTION : m√™me logique pour les erreurs de parsing
            extracted_text_final = extracted_text
            current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            response_data = {
                'success': False,
                'message': 'Contexte invalide.',
                'data': {
                    'user_domain': domaine,
                    'user_level': niveau,
                    'user_exercise_type': type_exercice,
                    'user_expectation': attente,
                    'user_info': infos,
                    'extracted_text': extracted_text_final
                }
            }
            CorrectionHistory.objects.create(
                user=request.user,
                user_domain=domaine,
                user_level=niveau,
                user_exercise_type=type_exercice,
                user_expectation=attente,
                user_info=infos,
                success=False,
                error_message='Contexte invalide.',
                extracted_text=extracted_text_final
            )
            return Response(response_data, status=status.HTTP_400_BAD_REQUEST)

        except Exception as e:
            logger.error(f"Erreur: {str(e)}")
            extracted_text_final = extracted_text
            current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            response_data = {
                'success': False,
                'message': f'Erreur serveur: {str(e)}',
                'data': {
                    'user_domain': domaine,
                    'user_level': niveau,
                    'user_exercise_type': type_exercice,
                    'user_expectation': attente,
                    'user_info': infos,
                    'extracted_text': extracted_text_final
                }
            }
            CorrectionHistory.objects.create(
                user=request.user,
                user_domain=domaine,
                user_level=niveau,
                user_exercise_type=type_exercice,
                user_expectation=attente,
                user_info=infos,
                success=False,
                error_message=f'Erreur serveur: {str(e)}',
                extracted_text=extracted_text_final
            )
            return Response(response_data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    def _extract_text(self, image_bytes):
        """Extrait le texte de l'image pour fallback"""
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            image_part = {'mime_type': 'image/jpeg', 'data': image_bytes}
            prompt = "Extrayez le texte brut de l'image sans interpr√©tation."
            response = model.generate_content([prompt, image_part], generation_config={"temperature": 0.1})
            return response.text.strip()[:1000]  # Limite pour √©viter surcharge
        except Exception as e:
            logger.error(f"Erreur extraction texte: {str(e)}")
            return ""

    def _detect_branch(self, image_bytes, extracted_text):
        """Analyse l'image et le texte extrait pour d√©tecter la branche"""
        model = genai.GenerativeModel('gemini-2.5-flash')
        try:
            image_part = {'mime_type': 'image/jpeg', 'data': image_bytes}
            prompt = f"""
Analyse l'image et le texte suivant pour d√©terminer si l'exercice est SCIENTIFIQUE (Math√©matiques, Physique-Chimie, SVT, Informatique, √âconomie/SES) ou LITT√âRAIRE (Fran√ßais, Histoire-G√©ographie, Philosophie, Langues √©trang√®res, Autre).
Texte extrait : {extracted_text[:1000]}
Retourne un JSON avec :
- "branch": "Scientifique" ou "Litt√©raire"
- "detected_domain": Le domaine sp√©cifique (ex. "Anglais", "Math√©matiques")
- "explanation": Une courte explication de la d√©tection

JSON :
{{
  "branch": "Scientifique ou Litt√©raire",
  "detected_domain": "Domaine d√©tect√©",
  "explanation": "Explication de la d√©tection"
}}
"""
            response = model.generate_content([prompt, image_part], generation_config={"temperature": 0.2})
            content = response.text.strip()
            try:
                result = json.loads(content)
                return {
                    'branch': result.get('branch', 'G√©n√©ral'),
                    'detected_domain': result.get('detected_domain', 'G√©n√©ral'),
                    'explanation': result.get('explanation', 'Analyse non concluante.')
                }
            except json.JSONDecodeError:
                # Fallback : Analyse du texte extrait
                return self._fallback_branch_detection(extracted_text)
        except Exception as e:
            logger.error(f"Erreur d√©tection branche: {str(e)}")
            return self._fallback_branch_detection(extracted_text)

    def _fallback_branch_detection(self, extracted_text):
        """Fallback bas√© sur le texte extrait"""
        text = extracted_text.lower()
        literary_keywords = ['english', 'fran√ßais', 'histoire', 'philosophie', 'langues', 'r√©daction', 'texte']
        scientific_keywords = ['math', 'physique', 'chimie', 'svt', 'informatique', '√©conomie', 'equation', 'calcul']
        
        if any(keyword in text for keyword in literary_keywords):
            return {
                'branch': 'Litt√©raire',
                'detected_domain': 'Langues √©trang√®res' if 'english' in text else 'Fran√ßais',
                'explanation': f"D√©tection bas√©e sur le texte extrait : mots-cl√©s litt√©raires d√©tect√©s ({'english' if 'english' in text else 'fran√ßais'})."
            }
        elif any(keyword in text for keyword in scientific_keywords):
            return {
                'branch': 'Scientifique',
                'detected_domain': 'Math√©matiques' if 'math' in text else 'Physique-Chimie',
                'explanation': f"D√©tection bas√©e sur le texte extrait : mots-cl√©s scientifiques d√©tect√©s ({'math' if 'math' in text else 'physique'})."
            }
        return {
            'branch': 'G√©n√©ral',
            'detected_domain': 'G√©n√©ral',
            'explanation': 'Aucun mot-cl√© sp√©cifique d√©tect√© dans le texte extrait.'
        }

    def _analyze_content_type_with_education(self, domaine, type_exercice, attente, niveau):
        """Analyse avec d√©tection Fair Use √©ducatif"""
        is_educational = niveau in self.EDUCATIONAL_LEVELS
        is_literary_sensitive = (domaine in self.LITERARY_DOMAINS and type_exercice in self.SENSITIVE_TYPES)
        
        if domaine in self.SCIENTIFIC_DOMAINS or type_exercice in ['QCM', 'Probl√®me √† r√©soudre', 'D√©monstration / raisonnement']:
            return {
                'type': 'SCIENTIFIC',
                'needs_latex': True,
                'safety_level': 'LOW',
                'temperature': 0.2,
                'educational_mode': is_educational
            }
        
        if is_literary_sensitive and is_educational:
            return {
                'type': 'LITERARY_EDUCATIONAL',
                'needs_latex': False,
                'safety_level': 'EDUCATIONAL',
                'temperature': 0.7,
                'educational_mode': True,
                'fair_use': True
            }
        
        if is_literary_sensitive:
            return {
                'type': 'LITERARY_SENSITIVE',
                'needs_latex': False,
                'safety_level': 'HIGH',
                'temperature': 0.8,
                'educational_mode': False
            }
        
        if domaine in self.LITERARY_DOMAINS:
            return {
                'type': 'LITERARY_MODERATE',
                'needs_latex': False,
                'safety_level': 'MEDIUM',
                'temperature': 0.6,
                'educational_mode': is_educational
            }
        
        return {
            'type': 'GENERAL',
            'needs_latex': False,
            'safety_level': 'MEDIUM',
            'temperature': 0.5,
            'educational_mode': is_educational
        }

    def _educational_fallback(self, domaine, type_exercice, niveau, analysis):
        """Fallback p√©dagogique si copyright bloqu√©"""
        logger.info("Mode fallback √©ducatif activ√©")
        return {
            'success': True,
            'educational_mode': True,
            'extracted_text': f"M√©thode p√©dagogique g√©n√©rale - {domaine} {niveau}",
            'result': f"R√©sultat Final: M√©thode d'analyse structur√©e pour {type_exercice}",
            'steps': [
                f"CONTEXTE P√âDAGOGIQUE {niveau}: ",
                "1. M√©thode g√©n√©rale d'analyse adapt√©e au niveau",
                "2. Structure recommand√©e pour ce type d'exercice",
                "3. Conseils m√©thodologiques pour r√©ussir",
                "4. Exemple g√©n√©rique d'application",
                "5. Points d'am√©lioration pour l'√©l√®ve"
            ]
        }

    def call_gemini_api(self, image_bytes, domaine, niveau, type_exercice, attente, infos, content_analysis):
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        safety_settings = self._get_educational_safety_settings(content_analysis)
        prompt = self._build_educational_prompt(content_analysis, domaine, niveau, type_exercice, attente, infos)

        try:
            image_part = {'mime_type': 'image/jpeg', 'data': image_bytes}
            
            response = model.generate_content(
                [prompt, image_part],
                safety_settings=safety_settings,
                generation_config={
                    "temperature": content_analysis['temperature'],
                    "top_p": 0.9,
                    "max_output_tokens": 5000
                }
            )

            candidate = response.candidates[0]
            
            if candidate.finish_reason == 4:  # COPYRIGHT_BLOCK
                logger.warning(f"Copyright √©ducatif bloqu√©: {content_analysis['type']}")
                return {
                    'success': False,
                    'message': 'Contenu sensible d√©tect√©. Mode p√©dagogique activ√©.',
                    'error_type': 'COPYRIGHT_BLOCK'
                }

            if candidate.finish_reason != 1 or not candidate.content.parts:
                return {
                    'success': False,
                    'message': 'R√©ponse invalide.',
                    'error_type': 'INVALID_RESPONSE'
                }

            content = response.text
            logger.info(f"Mode: {content_analysis.get('educational_mode', False)} | R√©ponse: {content[:150]}...")

            return self._parse_gemini_response(content, content_analysis['needs_latex'])

        except Exception as e:
            logger.error(f"Erreur Gemini: {str(e)}")
            return {'success': False, 'message': f'Erreur API: {str(e)}', 'error_type': 'API_ERROR'}

    def _get_educational_safety_settings(self, analysis):
        """Safety settings avec exception √©ducative"""
        base_settings = [
            {"category": HarmCategory.HARM_CATEGORY_HARASSMENT, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE},
            {"category": HarmCategory.HARM_CATEGORY_HATE_SPEECH, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE}
        ]
        
        safety_level = analysis['safety_level']
        
        if safety_level == 'EDUCATIONAL':
            base_settings.extend([
                {"category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE},
                {"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": HarmBlockThreshold.BLOCK_LOW_AND_ABOVE}
            ])
        elif safety_level == 'HIGH':
            base_settings.append({"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE})
        else:
            base_settings.append({"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": HarmBlockThreshold.BLOCK_ONLY_HIGH})
        
        return base_settings

    def _build_educational_prompt(self, analysis, domaine, niveau, type_exercice, attente, infos):
        """Prompt avec contexte √©ducatif Fair Use"""
        educational_mode = analysis.get('educational_mode', False)
        type_content = analysis['type']
        
        if type_content == 'SCIENTIFIC':
            return self._build_scientific_prompt(niveau, type_exercice, attente, analysis['needs_latex'])
        
        elif type_content == 'LITERARY_EDUCATIONAL':
            return self._build_educational_literary_prompt(niveau, type_exercice, attente, domaine)
        
        elif type_content == 'LITERARY_SENSITIVE':
            return self._build_literary_sensitive_prompt(niveau, type_exercice, attente)
        
        elif type_content == 'LITERARY_MODERATE':
            if educational_mode:
                return self._build_educational_literary_prompt(niveau, type_exercice, attente, domaine)
            return self._build_literary_moderate_prompt(niveau, type_exercice, attente)
        
        else:
            return self._build_general_prompt(niveau, type_exercice, attente)

    def _build_educational_literary_prompt(self, niveau, type_exercice, attente, domaine):
        """Prompt PROFESSEUR pour contenu litt√©raire √©ducatif ‚Äî m√™me structure JSON que scientifique"""
        return f"""
Assistant professeur certifi√© en {domaine} ‚Äî niveau {niveau}
Type d'exercice : {type_exercice} | Attente : {attente}

üéì Objectif :
Corriger l'exercice de fa√ßon p√©dagogique et m√©thodique, en respectant le cadre du fair use √©ducatif :
- Citations courtes (max 150 mots)
- Pas de reproduction int√©grale d'≈ìuvres
- Explication m√©thodologique et critique
- Correction structur√©e et compr√©hensible pour l'√©l√®ve

üìã Instructions :
1. Extraire le texte de l'image (reformul√© si n√©cessaire)
2. Fournir une correction compl√®te et p√©dagogique
3. Structurer la r√©ponse sous forme de JSON au format suivant :

JSON :
{{
  "extracted_text": "Texte avec LaTeX si √©quations",
  "result": "R√©sultat Final : correction de l'exercice ",
  "steps": ["Correction D√©taill√©es : ", "1. Analyse", "$$calculs$$", "2. R√©solution"]
}}

‚ö†Ô∏è IMPORTANT :
- R√©ponds uniquement avec le JSON valide ci-dessus.
- N‚Äôajoute aucun texte hors du JSON.
"""




    def _build_scientific_prompt(self, niveau, type_exercice, attente, needs_latex):
        latex_instruction = "Utilisez LaTeX $$pour √©quations$$ et $inline$." if needs_latex else ""
        return f"""
Assistant scientifique expert niveau {niveau}
Type: {type_exercice} | Attente: {attente}

1. Extrayez texte image avec {latex_instruction}
2. Solution d√©taill√©e avec calculs LaTeX
3. R√©sultat final clair

JSON :
{{
  "extracted_text": "Texte avec LaTeX si √©quations",
  "result": "R√©sultat Final : r√©ponse concise",
  "steps": ["Correction D√©taill√©es : ", "1. Analyse", "$$calculs$$", "2. R√©solution"]
}}
        """

    def _build_literary_sensitive_prompt(self, niveau, type_exercice, attente):
        return f"""
‚ö†Ô∏è CR√âATION 100% ORIGINALE - AUCUN COPYRIGHT ‚ö†Ô∏è
Niveau {niveau} | Type: {type_exercice}

1. Reformulez AVEC VOS MOTS
2. M√©thode originale d'analyse
3. Exemples FICTIFS

JSON :
{{
  "extracted_text": "Reformulation originale",
  "result": "M√©thode originale",
  "steps": ["√âtapes originales", "Exemple fictif"]
}}
        """

    def _build_literary_moderate_prompt(self, niveau, type_exercice, attente):
        return f"""
Assistant {niveau} - {type_exercice}
Cr√©ez contenu original :
1. Reformulation
2. Analyse originale
3. Exemples g√©n√©riques

JSON :
{{
  "extracted_text": "Reformulation",
  "result": "Analyse concise",
  "steps": ["1. M√©thode", "2. Exemple g√©n√©rique"]
}}
        """

    def _build_general_prompt(self, niveau, type_exercice, attente):
        return f"""
Assistant {niveau} - {type_exercice}
Solution originale.

JSON :
{{
  "extracted_text": "Texte extrait",
  "result": "R√©sultat final ou correction de l'exercice",
  "steps": ["√âtapes"]
}}
        """

   # ‚Üê CORRECTION : parsing plus robuste
    def _parse_gemini_response(self, content, needs_latex):
        """Parse robuste avec fallback en cas de JSON incomplet."""
        try:
            result = json.loads(content.strip())
        except json.JSONDecodeError:
            # Tentative de r√©cup√©ration du bloc JSON brut
            try:
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end > start:
                    result = json.loads(content[start:end])
                else:
                    raise
            except Exception:
                # Fallback ultime
                return {
                    'success': True,
                    'extracted_text': content[:500],
                    'result': 'R√©ponse partielle analys√©e',
                    'steps': [content[:1000]]
                }

        # Nettoyage LaTeX pour les domaines non-scientifiques
        if not needs_latex:
            result['result'] = result.get('result', '').replace('$\\text{', '').replace('}$', '')
            result['steps'] = [
                step.replace('$\\text{', '').replace('}$', '')
                for step in result.get('steps', [])
            ]

        return {
            'success': True,
            'extracted_text': result.get('extracted_text', content[:500]),
            'result': result.get('result', ''),
            'steps': result.get('steps', [])
        }


class HistoryView(APIView):
    permission_classes = [IsAuthenticated]
    def get(self, request):
        corrections = CorrectionHistory.objects.filter(user=request.user).order_by('-created_at')
        serializer = CorrectionHistorySerializer(corrections, many=True)
        return Response(serializer.data)



@api_view(['GET'])
def user_stats(request):
    from django.db.models import Count
    permission_classes = [IsAuthenticated]
    image_corrections = CorrectionHistory.objects.filter(
        user=request.user
    ).count()
    
    # Chat questions (si tu as d√©j√† des UsageLog)
    chat_questions = UsageLog.objects.filter(
        subscription__user=request.user,
        action='CHAT_QUESTION'
    ).count()

    return Response({
        'success': True,
        'data': {
            'used_image_corrections': image_corrections,
            'used_chat_questions': chat_questions,
        }
    })


# backend/views.py
import google.generativeai as genai
from rest_framework import generics, status
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated
from django.utils import timezone
from .models import ChatSession, ChatMessage
from .serializers import (
    ChatSessionListSerializer, ChatSessionDetailSerializer, ChatMessageSerializer
)

genai.configure(api_key=settings.GEMINI_API_KEY)

class ChatSessionListCreateView(generics.ListCreateAPIView):
    permission_classes = [IsAuthenticated]
    serializer_class = ChatSessionListSerializer

    def get_queryset(self):
        return ChatSession.objects.filter(user=self.request.user, is_active=True)

    def perform_create(self, serializer):
        # Cr√©er avec titre g√©n√©rique
        session = serializer.save(user=self.request.user)
        # Premier message IA
        ChatMessage.objects.create(
            session=session,
            role='assistant',
            content="Bonjour ! Posez-moi une question sur vos devoirs."
        )

class ChatSessionDetailView(generics.RetrieveAPIView):
    permission_classes = [IsAuthenticated]
    serializer_class = ChatSessionDetailSerializer

    def get_queryset(self):
        return ChatSession.objects.filter(user=self.request.user, is_active=True)
    
    
# views.py ‚Üí ChatMessageCreateView (VERSION M√âMOIRE ACTIVE)
class ChatMessageCreateView(generics.CreateAPIView):
    permission_classes = [IsAuthenticated]
    serializer_class = ChatMessageSerializer

    def create(self, request, *args, **kwargs):
        user = request.user
        session_id = request.data.get('session_id')
        user_message = request.data.get('content', '').strip()

        if not user_message:
            return Response({
                'success': False,
                'message': 'Contenu du message requis.'
            }, status=400)

        # === CR√âER SESSION SI ABSENTE ===
        if not session_id:
            session = ChatSession.objects.create(
                user=user,
                title="Nouvelle discussion"
            )
            # Message d'accueil
            ChatMessage.objects.create(
                session=session,
                role='model',
                content="Bonjour ! Posez-moi une question sur vos devoirs."
            )
        else:
            try:
                session = ChatSession.objects.get(id=session_id, user=user, is_active=True)
            except ChatSession.DoesNotExist:
                return Response({'success': False, 'message': 'Session introuvable.'}, status=404)

        # === V√âRIFIER QUOTA ===
        try:
            subscription = Subscription.objects.get(user=user, is_active=True)
            if subscription.chat_questions_remaining <= 0:
                return Response({
                    'success': False,
                    'message': 'Quota de questions √©puis√©. Passez √† un pack sup√©rieur.',
                    'upgrade_required': True
                }, status=403)
        except Subscription.DoesNotExist:
            return Response({
                'success': False,
                'message': 'Aucun abonnement actif.'
            }, status=403)

        # === SAUVEGARDER MESSAGE UTILISATEUR ===
        user_msg = ChatMessage.objects.create(
            session=session,
            role='user',
            content=user_message
        )

        # === APPEL GEMINI AVEC M√âMOIRE (LE COEUR DU CHATBOT) ===
        model = genai.GenerativeModel('gemini-2.5-flash')  # Plus rapide & meilleur contexte

        # R√©cup√®re TOUT l'historique
        history = session.messages.all().order_by('created_at')
        chat_history = []
        for msg in history:
            role = "user" if msg.role == 'user' else "model"
            chat_history.append({"role": role, "parts": [msg.content]})

        # D√©marre le chat avec m√©moire
        chat = model.start_chat(history=chat_history)

        # Envoie SEULEMENT le nouveau message
        try:
            start_time = timezone.now()
            response = chat.send_message(user_message)
            response_time = (timezone.now() - start_time).total_seconds() * 1000

            # Sauvegarde r√©ponse IA
            ai_msg = ChatMessage.objects.create(
                session=session,
                role='model',
                content=response.text,
                gemini_response_time=round(response_time, 2)
            )

            # D√©duit quota
            subscription.chat_questions_remaining -= 1
            subscription.save()

            # TITRE PRO PAR GEMINI
            if session.messages.filter(role='user').count() == 1:
                title_prompt = f"""
                Tu es un expert en titres accrocheurs.
                Voici le premier message de l'utilisateur : "{user_message}"
                
                G√©n√®re UN SEUL titre court (5-8 mots max), professionnel et p√©dagogique.
                Exemples :
                - "Racine carr√©e expliqu√©e simplement"
                - "Adverbes : 3 exemples cl√©s"
                - "Participe pass√© : r√®gle facile"
                
                R√©ponds UNIQUEMENT le titre, rien d'autre.
                """
                
                try:
                    title_response = model.generate_content(title_prompt)
                    suggested_title = title_response.text.strip()
                    
                    if len(suggested_title) > 60:
                        suggested_title = suggested_title[:57] + "..."
                        
                    session.title = suggested_title
                    session.save()
                    logger.info(f"Titre IA g√©n√©r√© : {suggested_title}")
                    
                except Exception as e:
                    logger.warning(f"√âchec titre IA : {e}")
                    fallback = user_message[:47] + ("..." if len(user_message) > 50 else "")
                    session.title = fallback
                    session.save()

            return Response({
                'success': True,
                'message': 'R√©ponse g√©n√©r√©e',
                'data': ChatMessageSerializer(ai_msg).data,
                'remaining_questions': subscription.chat_questions_remaining,
                'session_id': str(session.id)
            })

        except Exception as e:
            return Response({
                'success': False,
                'message': f'Erreur IA : {str(e)}'
            }, status=500)





# views.py
import os
from django.conf import settings
from rest_framework.decorators import api_view, permission_classes
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
import google.generativeai as genai
import base64

# treatment/views.py
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def test_local_and_save(request):
    filename = request.data.get('filename')
    if not filename:
        return Response({'success': False, 'message': 'filename requis'}, status=400)

    # === CHEMIN LOCAL ===
    file_path = os.path.join(settings.BASE_DIR, 'treatment', 'media', 'images', filename)
    if not os.path.exists(file_path):
        return Response({'success': False, 'message': f'Image non trouv√©e : {filename}'}, status=404)

    # === CONTEXTE ===
    domaine = request.data.get('domaine', 'Math√©matiques')
    niveau = request.data.get('niveau', 'Coll√®ge')
    type_ex = request.data.get('type_exercice', 'Probl√®me')
    attente = request.data.get('attente', '√âtape par √©tape')
    infos = request.data.get('infos', '')

    # === SAUVEGARDE DANS LA BASE (m√™me sans upload) ===
    # On copie l'image dans media/corrections/ pour l'historique
    from django.core.files import File
    import shutil
    from datetime import datetime

    # Cr√©e le dossier si besoin
    upload_dir = os.path.join(settings.MEDIA_ROOT, 'corrections', datetime.now().strftime('%Y/%m/%d'))
    os.makedirs(upload_dir, exist_ok=True)

    # Copie le fichier
    new_filename = f"{request.user.id}_{int(datetime.now().timestamp())}_{filename}"
    destination_path = os.path.join(upload_dir, new_filename)
    shutil.copy(file_path, destination_path)

    # Sauvegarde en base
    correction = ImageCorrection.objects.create(
        user=request.user,
        domaine=domaine,
        niveau=niveau,
        type_exercice=type_ex,
        attente=attente,
        infos_complementaires=infos,
        correction_text="Analyse en cours...",
    )
    # Associe l'image
    with open(destination_path, 'rb') as f:
        correction.image.save(new_filename, File(f), save=True)

    # === GEMINI VISION ===
    genai.configure(api_key=settings.GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash')

    with open(file_path, "rb") as img_file:
        image_bytes = img_file.read()

    # === CORRECT MIME TYPE ===
    ext = filename.split('.')[-1].lower()
    mime_type = {
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'webp': 'image/webp',
        'gif': 'image/gif',
    }.get(ext, 'image/jpeg')

    image_part = {
        "mime_type": mime_type,
        "data": base64.b64encode(image_bytes).decode('utf-8')
    }
    
    prompt = f"""
    CORRIGE CET EXERCICE PHOTO
    Mati√®re : {domaine}
    Niveau : {niveau}
    Type : {type_ex}
    Attente : {attente}
    Infos : {infos}

    R√©ponds en Markdown avec :
    - Question reconnue
    - Correction d√©taill√©e
    - R√©ponse finale en cadre
    """

    try:
        response = model.generate_content([prompt, image_part])
        correction.correction_text = response.text
        correction.save()

        return Response({
            'success': True,
            'correction_id': correction.id,
            'filename': filename,
            'correction': response.text,
            'image_url': correction.image.url,
            'saved_at': correction.created_at.isoformat()
        })

    except Exception as e:
        correction.correction_text = f"Erreur Gemini : {str(e)}"
        correction.save()
        return Response({'success': False, 'error': str(e)}, status=500)
    





# treatment/views.py
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def correct_and_upload(request):
    image_file = request.FILES.get('image')
    if not image_file:
        return Response({'success': False, 'message': 'Image requise'}, status=400)

    # === CONTEXTE ===
    domaine = request.data.get('domaine', 'Math√©matiques')
    niveau = request.data.get('niveau', 'Coll√®ge')
    type_ex = request.data.get('type_exercice', 'Probl√®me')
    attente = request.data.get('attente', '√âtape par √©tape')
    infos = request.data.get('infos', '')

    # === SAUVEGARDE IMM√âDIATE ===
    correction = ImageCorrection.objects.create(
        user=request.user,
        domaine=domaine,
        niveau=niveau,
        type_exercice=type_ex,
        attente=attente,
        infos_complementaires=infos,
        correction_text="Analyse en cours...",
        image=image_file,  # ‚Üê DIRECTEMENT !
    )

    # === GEMINI VISION ===
    genai.configure(api_key=settings.GEMINI_API_KEY)
    model = genai.GenerativeModel('gemini-2.5-flash')
    
    image_file.seek(0)  # ‚Üê REWINDE LE FICHIER
    image_bytes = image_file.read()
    ext = image_file.name.split('.')[-1].lower() if '.' in image_file.name else 'jpeg'
    mime_type = {
        'jpg': 'image/jpeg', 'jpeg': 'image/jpeg',
        'png': 'image/png', 'webp': 'image/webp', 'gif': 'image/gif'
    }.get(ext, 'image/jpeg')

    image_part = {
        "mime_type": mime_type,
        "data": base64.b64encode(image_bytes).decode('utf-8')
    }

    prompt = f"""
Tu es une intelligence artificielle experte en √©ducation, charg√©e de corriger des exercices √† partir d‚Äôune photo.  
Ta mission : fournir une r√©ponse claire, structur√©e et agr√©able √† lire sur mobile.

Voici le contexte :
- Mati√®re : {domaine}
- Niveau : {niveau}
- Type : {type_ex}
- Attente : {attente}
- Infos suppl√©mentaires : {infos}

R√©ponds uniquement en **Markdown lisible et esth√©tique**, dans le style de l‚Äôapplication mobile ChatGPT.

Structure attendue :
1. **Question reconnue** (si identifiable)
2. **Correction d√©taill√©e**, √©tape par √©tape ou par paragraphe clair
3. **Traite l'exercice **, bien int√©gr√©e au texte (pas de cadre obligatoire)

R√®gles de style Markdown :
- Utilise des titres (`##`, `###`) seulement si c‚Äôest utile.
- Mets les mots importants en **gras**.
- Utilise des listes √† puces ou num√©rot√©es pour les √©tapes.
- Utilise des blocs de code (```langage) pour les formules, calculs, ou extraits de texte.
- Un seul saut de ligne entre les paragraphes (√©vite les grands espaces).
- Pas de phrases inutiles comme ‚ÄúVoici la correction‚Äù ou ‚ÄúBien s√ªr !‚Äù.
- Ton : clair, p√©dagogique, fluide ‚Äî comme une vraie explication naturelle.

‚ö†Ô∏è Ne renvoie que du Markdown, sans HTML ni balises JSON.
"""

    try:
        response = model.generate_content([prompt, image_part])
        correction.correction_text = response.text
        correction.save()

        return Response({
            'success': True,
            'correction_id': correction.id,
            'correction': response.text,
            'image_url': correction.image.url,
            'saved_at': correction.created_at.isoformat()
        })

    except Exception as e:
        correction.correction_text = f"Erreur Gemini : {str(e)}"
        correction.save()
        return Response({'success': False, 'error': str(e)}, status=500)



# === HISTORIQUE DES CORRECTIONS PHOTO ===
@api_view(['GET'])
@permission_classes([IsAuthenticated])
def history_corrections(request):
    corrections = ImageCorrection.objects.filter(user=request.user).order_by('-created_at')
    data = []
    for c in corrections:
        data.append({
            'id': c.id,
            'domaine': c.domaine,
            'niveau': c.niveau,
            'type_exercice': c.type_exercice,
            'attente': c.attente,
            'infos': c.infos_complementaires,
            'correction': c.correction_text,
            'image_url': request.build_absolute_uri(c.image.url),
            'date': c.created_at.strftime("%d/%m/%Y %H:%M"),
        })
    
    return Response({
        'success': True,
        'total': corrections.count(),
        'history': data
    })



    
    
# import logging
# import google.generativeai as genai
# from django.conf import settings
# import json
# from rest_framework.views import APIView
# from rest_framework.parsers import MultiPartParser, FormParser
# from rest_framework import status
# from rest_framework.response import Response
# from google.generativeai.types import HarmCategory, HarmBlockThreshold
# from datetime import datetime
# from rest_framework.permissions import IsAuthenticated
# from .models import CorrectionHistory  # Import du mod√®le
# from .serializers import CorrectionHistorySerializer

# logger = logging.getLogger(__name__)
# genai.configure(api_key=settings.GEMINI_API_KEY)

# class ProcessImageView(APIView):
#     parser_classes = [MultiPartParser, FormParser]
#     permission_classes = [IsAuthenticated]

#     LITERARY_DOMAINS = [
#         'Fran√ßais', 'Histoire-G√©ographie', 'Philosophie', 
#         'Langues √©trang√®res', 'Autre'
#     ]
    
#     SENSITIVE_TYPES = [
#         'Exercice de r√©daction', 'Analyse de texte'
#     ]
    
#     SCIENTIFIC_DOMAINS = [
#         'Math√©matiques', 'Physique-Chimie', 'SVT', 
#         'Informatique', '√âconomie / SES'
#     ]

#     EDUCATIONAL_LEVELS = [
#         'Lyc√©e ‚Äì Terminale', 'Lyc√©e ‚Äì Premi√®re', 'Lyc√©e ‚Äì Seconde',
#         'Coll√®ge (6·µâ ‚Äì 3·µâ)', 'Sup√©rieur ‚Äì BTS / DUT', 'Sup√©rieur ‚Äì Licence'
#     ]

#     def post(self, request):
#         TEST_MODE = False
#         if TEST_MODE:
#             image_path = r'C:\git_project\CORRECTION APP BACKEND\treatment\images\testfr.png'
#             context_str = json.dumps({
#                 'domaine': 'Francais',
#                 'niveau': 'Lyc√©e ‚Äì Terminale',
#                 'type_exercice': 'Probl√®me √† r√©soudre',
#                 'attente': 'Solution √©tape par √©tape',
#                 'infos': ''
#             })
#             try:
#                 with open(image_path, 'rb') as f:
#                     image_bytes = f.read()
#             except FileNotFoundError:
#                 logger.error(f"Image locale non trouv√©e: {image_path}")
#                 return Response(
#                     {'success': False, 'message': f'Image locale non trouv√©e: {image_path}'},
#                     status=status.HTTP_400_BAD_REQUEST
#                 )
#         else:
#             image = request.FILES.get('image')
#             context_str = request.POST.get('context')

#             if not image:
#                 return Response(
#                     {'success': False, 'message': 'Aucune image fournie.'},
#                     status=status.HTTP_400_BAD_REQUEST
#                 )
#             image_bytes = image.read()

#         try:
#             context = json.loads(context_str) if context_str else {}
#             domaine = context.get('domaine', 'Math√©matiques')
#             niveau = context.get('niveau', 'Coll√®ge (6·µâ ‚Äì 3·µâ)')
#             type_exercice = context.get('type_exercice', 'Probl√®me √† r√©soudre')
#             attente = context.get('attente', 'Solution √©tape par √©tape')
#             infos = context.get('infos', '')
            
#             logger.info(f"Contexte: Domaine={domaine}, Type={type_exercice}, Niveau={niveau}")

#             # ‚úÖ EXTRACTION INITIALE DU TEXTE POUR FALLBACK
#             extracted_text = self._extract_text(image_bytes)
            
#             # ‚úÖ ANALYSE INITIALE DE LA BRANCHE PAR L'IA
#             branch_analysis = self._detect_branch(image_bytes, extracted_text)
            
#             # ‚úÖ V√âRIFICATION DE L'INCOH√âRENCE DOMAINE/BRANCHE
#             detected_branch = branch_analysis['branch']
#             detected_domain = branch_analysis['detected_domain']
#             domain_mismatch = None
#             if domaine != detected_domain:
#                 domain_mismatch = f"L'utilisateur a indiqu√© '{domaine}', mais l'exercice est d√©tect√© comme '{detected_domain}' ({detected_branch})."

#             # ‚úÖ ANALYSE DU TYPE DE CONTENU
#             content_analysis = self._analyze_content_type_with_education(
#                 detected_domain, type_exercice, attente, niveau
#             )
            
#             ia_response = self.call_gemini_api(
#                 image_bytes, detected_domain, niveau, type_exercice, 
#                 attente, infos, content_analysis
#             )

#             # ‚úÖ AJOUT DATE/HEURE BACKEND
#             current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#             # Pr√©parer la r√©ponse JSON
#             response_data = {
#                 'success': True,
#                 'data': {
#                     'user_domain': domaine,
#                     'user_level': niveau,
#                     'user_exercise_type': type_exercice,
#                     'user_expectation': attente,
#                     'user_info': infos,
#                     'detected_branch': detected_branch,
#                     'detected_branch_explanation': branch_analysis['explanation'],
#                     'domain_mismatch': domain_mismatch,
#                     'response_datetime': current_datetime,
#                     'extracted_text': ia_response.get('extracted_text', extracted_text),
#                     'solution': {
#                         'result': ia_response.get('result', ''),
#                         'steps': ia_response.get('steps', [])
#                     }
#                 },
#                 'content_type': content_analysis['type'],
#                 'educational_mode': content_analysis.get('educational_mode', False)
#             }

#             if not ia_response.get('success', True):
#                 if ia_response.get('error_type') == 'COPYRIGHT_BLOCK':
#                     ia_response = self._educational_fallback(
#                         detected_domain, type_exercice, niveau, content_analysis
#                     )
                
#                 if not ia_response.get('success', True):
#                     response_data = {
#                         'success': False,
#                         'message': ia_response['message'],
#                         'data': {
#                             'user_domain': domaine,
#                             'user_level': niveau,
#                             'user_exercise_type': type_exercice,
#                             'user_expectation': attente,
#                             'user_info': infos,
#                             'solution': None,
#                             'detected_branch': detected_branch,
#                             'detected_branch_explanation': branch_analysis['explanation'],
#                             'domain_mismatch': domain_mismatch
#                         },
#                         'content_type': content_analysis['type'],
#                         'detected_branch': detected_branch,
#                         'detected_branch_explanation': branch_analysis['explanation']
#                     }

#             # ‚úÖ SAUVEGARDE DANS L'HISTORIQUE
#             CorrectionHistory.objects.create(
#                 user=request.user,
#                 user_domain=domaine,
#                 user_level=niveau,
#                 user_exercise_type=type_exercice,
#                 user_expectation=attente,
#                 user_info=infos,
#                 detected_branch=detected_branch,
#                 detected_branch_explanation=branch_analysis['explanation'],
#                 domain_mismatch=domain_mismatch,
#                 response_datetime=current_datetime,
#                 extracted_text=response_data['data']['extracted_text'],
#                 solution=response_data['data']['solution'],
#                 content_type=content_analysis['type'],
#                 educational_mode=content_analysis.get('educational_mode', False),
#                 success=response_data['success'],
#                 error_message=response_data.get('message', None)
#             )

#             return Response(response_data)

#         except json.JSONDecodeError:
#             response_data = {
#                 'success': False,
#                 'message': 'Contexte invalide.',
#                 'data': {
#                     'user_domain': domaine,
#                     'user_level': niveau,
#                     'user_exercise_type': type_exercice,
#                     'user_expectation': attente,
#                     'user_info': infos
#                 }
#             }
#             # ‚úÖ SAUVEGARDE DANS L'HISTORIQUE (ERREUR)
#             CorrectionHistory.objects.create(
#                 user=request.user,
#                 user_domain=domaine,
#                 user_level=niveau,
#                 user_exercise_type=type_exercice,
#                 user_expectation=attente,
#                 user_info=infos,
#                 success=False,
#                 error_message='Contexte invalide.'
#             )
#             return Response(response_data, status=status.HTTP_400_BAD_REQUEST)
#         except Exception as e:
#             logger.error(f"Erreur: {str(e)}")
#             response_data = {
#                 'success': False,
#                 'message': f'Erreur serveur: {str(e)}',
#                 'data': {
#                     'user_domain': domaine,
#                     'user_level': niveau,
#                     'user_exercise_type': type_exercice,
#                     'user_expectation': attente,
#                     'user_info': infos
#                 }
#             }
#             # ‚úÖ SAUVEGARDE DANS L'HISTORIQUE (ERREUR)
#             CorrectionHistory.objects.create(
#                 user=request.user,
#                 user_domain=domaine,
#                 user_level=niveau,
#                 user_exercise_type=type_exercice,
#                 user_expectation=attente,
#                 user_info=infos,
#                 success=False,
#                 error_message=f'Erreur serveur: {str(e)}'
#             )
#             return Response(response_data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)

#     def _extract_text(self, image_bytes):
#         """Extrait le texte de l'image pour fallback"""
#         try:
#             model = genai.GenerativeModel('gemini-2.5-flash')
#             image_part = {'mime_type': 'image/jpeg', 'data': image_bytes}
#             prompt = "Extrayez le texte brut de l'image sans interpr√©tation."
#             response = model.generate_content([prompt, image_part], generation_config={"temperature": 0.1})
#             return response.text.strip()[:500]  # Limite pour √©viter surcharge
#         except Exception as e:
#             logger.error(f"Erreur extraction texte: {str(e)}")
#             return ""

#     def _detect_branch(self, image_bytes, extracted_text):
#         """Analyse l'image et le texte extrait pour d√©tecter la branche"""
#         model = genai.GenerativeModel('gemini-2.5-flash')
#         try:
#             image_part = {'mime_type': 'image/jpeg', 'data': image_bytes}
#             prompt = f"""
# Analyse l'image et le texte suivant pour d√©terminer si l'exercice est SCIENTIFIQUE (Math√©matiques, Physique-Chimie, SVT, Informatique, √âconomie/SES) ou LITT√âRAIRE (Fran√ßais, Histoire-G√©ographie, Philosophie, Langues √©trang√®res, Autre).
# Texte extrait : {extracted_text[:500]}
# Retourne un JSON avec :
# - "branch": "Scientifique" ou "Litt√©raire"
# - "detected_domain": Le domaine sp√©cifique (ex. "Anglais", "Math√©matiques")
# - "explanation": Une courte explication de la d√©tection

# JSON :
# {{
#   "branch": "Scientifique ou Litt√©raire",
#   "detected_domain": "Domaine d√©tect√©",
#   "explanation": "Explication de la d√©tection"
# }}
# """
#             response = model.generate_content([prompt, image_part], generation_config={"temperature": 0.2})
#             content = response.text.strip()
#             try:
#                 result = json.loads(content)
#                 return {
#                     'branch': result.get('branch', 'G√©n√©ral'),
#                     'detected_domain': result.get('detected_domain', 'G√©n√©ral'),
#                     'explanation': result.get('explanation', 'Analyse non concluante.')
#                 }
#             except json.JSONDecodeError:
#                 # Fallback : Analyse du texte extrait
#                 return self._fallback_branch_detection(extracted_text)
#         except Exception as e:
#             logger.error(f"Erreur d√©tection branche: {str(e)}")
#             return self._fallback_branch_detection(extracted_text)

#     def _fallback_branch_detection(self, extracted_text):
#         """Fallback bas√© sur le texte extrait"""
#         text = extracted_text.lower()
#         literary_keywords = ['english', 'fran√ßais', 'histoire', 'philosophie', 'langues', 'r√©daction', 'texte']
#         scientific_keywords = ['math', 'physique', 'chimie', 'svt', 'informatique', '√©conomie', 'equation', 'calcul']
        
#         if any(keyword in text for keyword in literary_keywords):
#             return {
#                 'branch': 'Litt√©raire',
#                 'detected_domain': 'Langues √©trang√®res' if 'english' in text else 'Fran√ßais',
#                 'explanation': f"D√©tection bas√©e sur le texte extrait : mots-cl√©s litt√©raires d√©tect√©s ({'english' if 'english' in text else 'fran√ßais'})."
#             }
#         elif any(keyword in text for keyword in scientific_keywords):
#             return {
#                 'branch': 'Scientifique',
#                 'detected_domain': 'Math√©matiques' if 'math' in text else 'Physique-Chimie',
#                 'explanation': f"D√©tection bas√©e sur le texte extrait : mots-cl√©s scientifiques d√©tect√©s ({'math' if 'math' in text else 'physique'})."
#             }
#         return {
#             'branch': 'G√©n√©ral',
#             'detected_domain': 'G√©n√©ral',
#             'explanation': 'Aucun mot-cl√© sp√©cifique d√©tect√© dans le texte extrait.'
#         }

#     def _analyze_content_type_with_education(self, domaine, type_exercice, attente, niveau):
#         """Analyse avec d√©tection Fair Use √©ducatif"""
#         is_educational = niveau in self.EDUCATIONAL_LEVELS
#         is_literary_sensitive = (domaine in self.LITERARY_DOMAINS and type_exercice in self.SENSITIVE_TYPES)
        
#         if domaine in self.SCIENTIFIC_DOMAINS or type_exercice in ['QCM', 'Probl√®me √† r√©soudre', 'D√©monstration / raisonnement']:
#             return {
#                 'type': 'SCIENTIFIC',
#                 'needs_latex': True,
#                 'safety_level': 'LOW',
#                 'temperature': 0.2,
#                 'educational_mode': is_educational
#             }
        
#         if is_literary_sensitive and is_educational:
#             return {
#                 'type': 'LITERARY_EDUCATIONAL',
#                 'needs_latex': False,
#                 'safety_level': 'EDUCATIONAL',
#                 'temperature': 0.7,
#                 'educational_mode': True,
#                 'fair_use': True
#             }
        
#         if is_literary_sensitive:
#             return {
#                 'type': 'LITERARY_SENSITIVE',
#                 'needs_latex': False,
#                 'safety_level': 'HIGH',
#                 'temperature': 0.8,
#                 'educational_mode': False
#             }
        
#         if domaine in self.LITERARY_DOMAINS:
#             return {
#                 'type': 'LITERARY_MODERATE',
#                 'needs_latex': False,
#                 'safety_level': 'MEDIUM',
#                 'temperature': 0.6,
#                 'educational_mode': is_educational
#             }
        
#         return {
#             'type': 'GENERAL',
#             'needs_latex': False,
#             'safety_level': 'MEDIUM',
#             'temperature': 0.5,
#             'educational_mode': is_educational
#         }

#     def _educational_fallback(self, domaine, type_exercice, niveau, analysis):
#         """Fallback p√©dagogique si copyright bloqu√©"""
#         logger.info("Mode fallback √©ducatif activ√©")
#         return {
#             'success': True,
#             'educational_mode': True,
#             'extracted_text': f"M√©thode p√©dagogique g√©n√©rale - {domaine} {niveau}",
#             'result': f"R√©sultat Final: M√©thode d'analyse structur√©e pour {type_exercice}",
#             'steps': [
#                 f"CONTEXTE P√âDAGOGIQUE {niveau}: ",
#                 "1. M√©thode g√©n√©rale d'analyse adapt√©e au niveau",
#                 "2. Structure recommand√©e pour ce type d'exercice",
#                 "3. Conseils m√©thodologiques pour r√©ussir",
#                 "4. Exemple g√©n√©rique d'application",
#                 "5. Points d'am√©lioration pour l'√©l√®ve"
#             ]
#         }

#     def call_gemini_api(self, image_bytes, domaine, niveau, type_exercice, attente, infos, content_analysis):
#         model = genai.GenerativeModel('gemini-2.5-flash')
        
#         safety_settings = self._get_educational_safety_settings(content_analysis)
#         prompt = self._build_educational_prompt(content_analysis, domaine, niveau, type_exercice, attente, infos)

#         try:
#             image_part = {'mime_type': 'image/jpeg', 'data': image_bytes}
            
#             response = model.generate_content(
#                 [prompt, image_part],
#                 safety_settings=safety_settings,
#                 generation_config={
#                     "temperature": content_analysis['temperature'],
#                     "top_p": 0.9,
#                     "max_output_tokens": 5000
#                 }
#             )

#             candidate = response.candidates[0]
            
#             if candidate.finish_reason == 4:  # COPYRIGHT_BLOCK
#                 logger.warning(f"Copyright √©ducatif bloqu√©: {content_analysis['type']}")
#                 return {
#                     'success': False,
#                     'message': 'Contenu sensible d√©tect√©. Mode p√©dagogique activ√©.',
#                     'error_type': 'COPYRIGHT_BLOCK'
#                 }

#             if candidate.finish_reason != 1 or not candidate.content.parts:
#                 return {
#                     'success': False,
#                     'message': 'R√©ponse invalide.',
#                     'error_type': 'INVALID_RESPONSE'
#                 }

#             content = response.text
#             logger.info(f"Mode: {content_analysis.get('educational_mode', False)} | R√©ponse: {content[:150]}...")

#             return self._parse_gemini_response(content, content_analysis['needs_latex'])

#         except Exception as e:
#             logger.error(f"Erreur Gemini: {str(e)}")
#             return {'success': False, 'message': f'Erreur API: {str(e)}', 'error_type': 'API_ERROR'}

#     def _get_educational_safety_settings(self, analysis):
#         """Safety settings avec exception √©ducative"""
#         base_settings = [
#             {"category": HarmCategory.HARM_CATEGORY_HARASSMENT, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE},
#             {"category": HarmCategory.HARM_CATEGORY_HATE_SPEECH, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE}
#         ]
        
#         safety_level = analysis['safety_level']
        
#         if safety_level == 'EDUCATIONAL':
#             base_settings.extend([
#                 {"category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE},
#                 {"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": HarmBlockThreshold.BLOCK_LOW_AND_ABOVE}
#             ])
#         elif safety_level == 'HIGH':
#             base_settings.append({"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE})
#         else:
#             base_settings.append({"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": HarmBlockThreshold.BLOCK_ONLY_HIGH})
        
#         return base_settings

#     def _build_educational_prompt(self, analysis, domaine, niveau, type_exercice, attente, infos):
#         """Prompt avec contexte √©ducatif Fair Use"""
#         educational_mode = analysis.get('educational_mode', False)
#         type_content = analysis['type']
        
#         if type_content == 'SCIENTIFIC':
#             return self._build_scientific_prompt(niveau, type_exercice, attente, analysis['needs_latex'])
        
#         elif type_content == 'LITERARY_EDUCATIONAL':
#             return self._build_educational_literary_prompt(niveau, type_exercice, attente, domaine)
        
#         elif type_content == 'LITERARY_SENSITIVE':
#             return self._build_literary_sensitive_prompt(niveau, type_exercice, attente)
        
#         elif type_content == 'LITERARY_MODERATE':
#             if educational_mode:
#                 return self._build_educational_literary_prompt(niveau, type_exercice, attente, domaine)
#             return self._build_literary_moderate_prompt(niveau, type_exercice, attente)
        
#         else:
#             return self._build_general_prompt(niveau, type_exercice, attente)

#     def _build_educational_literary_prompt(self, niveau, type_exercice, attente, domaine):
#         """Prompt PROFESSEUR pour contenu litt√©raire √©ducatif ‚Äî m√™me structure JSON que scientifique"""
#         return f"""
# Assistant professeur certifi√© en {domaine} ‚Äî niveau {niveau}
# Type d'exercice : {type_exercice} | Attente : {attente}

# üéì Objectif :
# Corriger l'exercice de fa√ßon p√©dagogique et m√©thodique, en respectant le cadre du fair use √©ducatif :
# - Citations courtes (max 150 mots)
# - Pas de reproduction int√©grale d'≈ìuvres
# - Explication m√©thodologique et critique
# - Correction structur√©e et compr√©hensible pour l'√©l√®ve

# üìã Instructions :
# 1. Extraire le texte de l'image (reformul√© si n√©cessaire)
# 2. Fournir une correction compl√®te et p√©dagogique
# 3. Structurer la r√©ponse sous forme de JSON au format suivant :

# JSON :
# {{
#   "extracted_text": "Texte avec LaTeX si √©quations",
#   "result": "R√©sultat Final : r√©ponse concise",
#   "steps": ["Correction D√©taill√©es : ", "1. Analyse", "$$calculs$$", "2. R√©solution"]
# }}

# ‚ö†Ô∏è IMPORTANT :
# - R√©ponds uniquement avec le JSON valide ci-dessus.
# - N‚Äôajoute aucun texte hors du JSON.
# """




#     def _build_scientific_prompt(self, niveau, type_exercice, attente, needs_latex):
#         latex_instruction = "Utilisez LaTeX $$pour √©quations$$ et $inline$." if needs_latex else ""
#         return f"""
# Assistant scientifique expert niveau {niveau}
# Type: {type_exercice} | Attente: {attente}

# 1. Extrayez texte image avec {latex_instruction}
# 2. Solution d√©taill√©e avec calculs LaTeX
# 3. R√©sultat final clair

# JSON :
# {{
#   "extracted_text": "Texte avec LaTeX si √©quations",
#   "result": "R√©sultat Final : r√©ponse concise",
#   "steps": ["Correction D√©taill√©es : ", "1. Analyse", "$$calculs$$", "2. R√©solution"]
# }}
#         """

#     def _build_literary_sensitive_prompt(self, niveau, type_exercice, attente):
#         return f"""
# ‚ö†Ô∏è CR√âATION 100% ORIGINALE - AUCUN COPYRIGHT ‚ö†Ô∏è
# Niveau {niveau} | Type: {type_exercice}

# 1. Reformulez AVEC VOS MOTS
# 2. M√©thode originale d'analyse
# 3. Exemples FICTIFS

# JSON :
# {{
#   "extracted_text": "Reformulation originale",
#   "result": "M√©thode originale",
#   "steps": ["√âtapes originales", "Exemple fictif"]
# }}
#         """

#     def _build_literary_moderate_prompt(self, niveau, type_exercice, attente):
#         return f"""
# Assistant {niveau} - {type_exercice}
# Cr√©ez contenu original :
# 1. Reformulation
# 2. Analyse originale
# 3. Exemples g√©n√©riques

# JSON :
# {{
#   "extracted_text": "Reformulation",
#   "result": "Analyse concise",
#   "steps": ["1. M√©thode", "2. Exemple g√©n√©rique"]
# }}
#         """

#     def _build_general_prompt(self, niveau, type_exercice, attente):
#         return f"""
# Assistant {niveau} - {type_exercice}
# Solution originale.

# JSON :
# {{
#   "extracted_text": "Texte extrait",
#   "result": "R√©sultat final",
#   "steps": ["√âtapes"]
# }}
#         """

#     def _parse_gemini_response(self, content, needs_latex):
#         """Parse robuste"""
#         try:
#             result = json.loads(content.strip())
#             # Supprimer LaTeX pour les exercices litt√©raires
#             if not needs_latex:
#                 result['result'] = result.get('result', '').replace('$\\text{', '').replace('}$', '')
#                 result['steps'] = [step.replace('$\\text{', '').replace('}$', '') for step in result.get('steps', [])]
#             return {
#                 'success': True,
#                 'extracted_text': result.get('extracted_text', ''),
#                 'result': result.get('result', ''),
#                 'steps': result.get('steps', [])
#             }
#         except json.JSONDecodeError:
#             start, end = content.find('{'), content.rfind('}') + 1
#             if start != -1 and end > start:
#                 try:
#                     result = json.loads(content[start:end])
#                     if not needs_latex:
#                         result['result'] = result.get('result', '').replace('$\\text{', '').replace('}$', '')
#                         result['steps'] = [step.replace('$\\text{', '').replace('}$', '') for step in result.get('steps', [])]
#                     return {
#                         'success': True,
#                         'extracted_text': result.get('extracted_text', content[:200]),
#                         'result': result.get('result', ''),
#                         'steps': result.get('steps', [content[:500]])
#                     }
#                 except:
#                     pass
        
#         return {
#             'success': True,
#             'extracted_text': content[:300],
#             'result': 'R√©ponse analys√©e',
#             'steps': [content[:800]]
#         }



# class HistoryView(APIView):
#     permission_classes = [IsAuthenticated]

#     def get(self, request):
#         corrections = CorrectionHistory.objects.filter(user=request.user).order_by('-created_at')
#         serializer = CorrectionHistorySerializer(corrections, many=True)
#         return Response(serializer.data)

